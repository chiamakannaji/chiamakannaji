{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Exercise 6: Set Generation\n",
    "\n",
    "Use a one line python list comprehension to generate a list with the powers of 2 set:\n",
    "\n",
    "$$S = \\{x \\in 1, 2, 4, 8, 16, 32, 64, ... | x < 562949953421312 \\}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 6"
   ]
  },
  {
   "source": [
    "## Exercise 6: Line of Best Fit\n",
    "\n",
    "In school, you probably worked with scatterplots and drew lines of best fit on the data that didn't deviate too far from the scatterpoints. By doing it this way, we would minimize the **Error** of the line of best fit.\n",
    "\n",
    "It is important to choose the model that will generate the least errors when fitted to your data. This will ensure far more reliable outputs. \n",
    "\n",
    "![image.png](../assets/ex6-line-best-fit.png)\n",
    "\n",
    "There are many error functions to choose from when it comes to modelling data. In this exercise , we will look at *Ordinary Least Squares*. As you can see in the picture, the vertical distance from the line of best fit to our data point is called the error. If we square the errors and take an average of the errors, we have obtained the **Mean Square Error (MSE)** for the model.\n",
    "\n",
    "For example, given\n",
    "\n",
    "- a line of best fit with $\\hat y = mx_i$.\n",
    "- data points $(-2,5)$, $(0,0)$, $(3,-6)$.\n",
    "\n",
    " We can compute the MSE:\n",
    "\n",
    " $$ MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat y)^2$$\n",
    "\n",
    "where $n$ is the number of data points, $y_i$ is the $y$ coordinate of the data point and $hat y$ is the output the line of best fit predicts for that specific $x$ coordinate.\n",
    "\n",
    "$$ MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - mx_i)^2$$\n",
    "\n",
    "Let's plug our data points into the error function for the model.\n",
    "\n",
    "$$ MSE = \\frac {1}{3}[(5-(-2)m)^2 + (0-(0)m)^2 + (-6-3m)^2]$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 6.1\n",
    "\n",
    "Use code to figure out which value of $m$ will minimize our error function. By hand, try to expand the equation for the MSE that we found above and clean it up. Then write it as a function in the cell below.\n",
    "\n",
    "Remember to format it as:\n",
    "\n",
    "```\n",
    "def f(x):\n",
    "    return...\n",
    "```\n",
    "\n",
    "When defining your function, use `x` instead of $m$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# exercise 6.1\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Exercise 6.2\n",
    "\n",
    "1. Use the `linspace` function to create 29 points between -28 and save the result as `W`.\n",
    "2. After that, divide every element in W by 13 and save this new result as `W`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 6.2\n"
   ]
  },
  {
   "source": [
    "### Exercise 6.3\n",
    "\n",
    "Run $f(W)$ and $fprime(W)$ in the cell below and determine the value in `W` that makes the `fprime = 0` (or very close to it!).\n",
    "\n",
    "Use the loop that you wrote in the previous exercise. That value should give us the value of $m$ that makes it so our line of best fit has the smallest error.\n",
    "\n",
    "NOTE: Make the print statement in your loop as \"The value of m that gives the smallest error is...\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 6.3\n"
   ]
  },
  {
   "source": [
    "## Exercise 7: Multivariate Calculus + Linear Algebra\n",
    "\n",
    "Up to now, we've looked at functions with respect to one variable, but what if we have more than one variable in our function and we want to take a derivative?\n",
    "\n",
    "Going back to our Error function exercise from the line of best fit, what if I wanted to fit the line:\n",
    "\n",
    "$$\\hat y = mx_i + b$$\n",
    "to the points (-3,7), (-2,5) and (-1,3).\n",
    "\n",
    "This would give a Mean Square Error function as:\n",
    "\n",
    "$$ f(m,b) = \\frac{1}{n}\\sum_{i=1}^n (y_i - mx_i - b)^2$$\n",
    "$$f(m,b) = \\frac {1}{3}[(7+3m-b)^2 + (5+2m-b)^2 + (3+m-b)^2]$$\n",
    "\n",
    "and say we wanted to find values of $m$ and $b$ that minimized this function. In this case, we'd apply a **partial derivative**. In other words, a derivative with respect to one of the variables holding the other constant. If we take derivatives of the above function with respect to $m$ and $b$, we get:\n",
    "\n",
    "$$\\frac{\\partial f(m,b)}{\\partial m} = \\frac{2}{3}[(7+3m-b)(3) + (5+2m-b)(2) + (3+m-b)] $$\n",
    "\n",
    "$$\\frac{\\partial f(m,b)}{\\partial b} = \\frac{2}{3}[(7+3m-b)(-1) + (5+2m-b)(-1) + (3+m-b)(-1)] $$\n",
    "\n",
    "> To better understand how we obtained these derivatives by hand, watch [this video](https://youtu.be/TgIl15Nlg_U) for a more detailed explanation.\n",
    "\n",
    "Let's clean up the above equations:\n",
    "\n",
    "$$\\frac{\\partial f(m,b)}{\\partial m} = \\frac{2}{3}[34 + 14m - 6b] $$\n",
    "\n",
    "$$\\frac{\\partial f(m,b)}{\\partial b} = \\frac{2}{3}[-15 -6m + 3b] $$\n",
    "\n",
    "Equating the partial derivatives to 0 since we want to obtain a minimum and multiplying both sides by $\\frac{3}{2}$ we get a familiar system of equations:\n",
    "\n",
    "$$34 + 14m - 6b = 0$$\n",
    "\n",
    "$$-15-6m+3b = 0$$\n",
    "\n",
    "Converting to matrix form, we get:\n",
    "\n",
    "$$\\begin{bmatrix} 34 & 14 & -6 \\\\ -15 & -6 & 3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ m \\\\ b \\end{bmatrix} = \\begin {bmatrix} 0\\\\ 0\\end{bmatrix} $$\n",
    "\n",
    "From here, we can use our standard matrix operations to solve for values of $m$ and $b$.\n",
    "\n",
    "We can rewrite the above equation as:\n",
    "\n",
    "$$\\begin{bmatrix} 14 & -6 \\\\ -6 & 3 \\end{bmatrix} \\begin{bmatrix} m \\\\ b \\end{bmatrix} = \\begin {bmatrix} -34\\\\ 15\\end{bmatrix} $$\n",
    "\n",
    "**EXTRA** Try to workout by hand how I was able to make the conversion between the two matrices. \n",
    "\n",
    "Use the cell below to write code that will solve the above matrix for the values of $m$ and $b$ that minimize our error."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 7\n"
   ]
  },
  {
   "source": [
    "_Using this method, we can fit more complicated models that have more than one parameter to our data for better results!_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}